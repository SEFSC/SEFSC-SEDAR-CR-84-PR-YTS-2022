---
title: "Model Diagnostics"
---

Model diagnostics aimed to follow the conceptual process described by @carvalho2021. Their approach includes evaluating goodness of fit, information sources and structure, prediction skill, convergence, and model plausibility. Although @carvalho2021 advise detours and additional model explorations when initial diagnostic tests fail, advanced diagnostics, such as likelihood profiles, retrospective, and jitter analyses, were conducted even when initial tests failed to comprehensively communicate the various model configurations explored to the extent possible.

## Convergence

Three approaches were used to check for model convergence. They were investigating for the presence of (1) bounded parameters, (2) high final gradients, and (3) a positive definite hessian. As described by @carvalho2021 checking for bounded parameters can indicate discrepancies with data or model structure. Additionally, small final gradients and a positive definite hessian can indicate that the objective function achieved good convergence.

The models presented in this report all had a positive definite Hessian, indicating that each reached a local minimum and a locally optimal fit. None of the models had parameters that were bounded, suggesting the optimization was not constrained by parameter limits. Finally, the parameter gradients in all models were small and well below 0.001, which is commonly used in the R4SS R package to identify large gradients (@tbl-parm).

{{< pagebreak >}}

## Correlation Analysis

High correlation among parameters can lead to flat response surfaces and poor model stability. By performing a correlation analysis, modeling assumptions that lead to inadequate configurations can be identified. Because of the highly parameterized nature of stock assessment models, some parameters are expected to be correlated (e.g., stock recruit parameters). However, many strongly correlated parameters suggest reconsidering modeling assumptions and parameterization.

High correlations (correlation coefficients greater than 0.95 or less than -0.95) were observed between selectivity parameters for the v08_m2, v28_m2, v28_m3, and v31_m3 (@tbl-corr). One particularly noteworthy correlation was between the estimates of initial fishing mortality (Initial F) and unfished recruitment (R0) in model v19_m2.

In the initial default configurations of both the m1 and m2 model scenarios, the standard error on the initial equilibrium catch was fixed at a low value of 0.01. This tightly constrained the model to the input catch of 168.3 metric tons effectively limiting flexibility in estimating the corresponding initial fishing mortality. To address this issue, the standard error was increased to 0.3, allowing the estimated initial catch to diverge from the fixed input value (@tbl-eqcatch). This adjustment reduced the overly strong correlation between Initial F and R0 by relaxing the constraint on initial fishing mortality. The effects of increasing the standard error beyond 0.3 are discussed further in the sensitivity analyses section.

All model scenarios showed moderate or high correlations between the  parameters used to define the some of the selectivity parameters, except 19_v3. Correlations between these selectivity parameters is expected. While estimated values varied slightly among models, they produced similar size-based selectivity curves for the commercial fleet (@fig-m3-selectivity). Time blocks applied in model v28_m3 and v31_m3 resulted with lower  peaks in the first block (1983-2003), higher peak in the second block (2004-2010) for the commercial selectivity pattern (@fig-selectivity-block). The selectivity for the NCRMP survey differed between across the models, particularly with regard to the end selectivity. These differences, in addition to the correlations NCRMP selectivity parameters, highlight that the estimated parameters informing the NCRMP selectivity relationship are not strongly informed by the available data. 

## Evaluating Variance

To check for parameters with high variance, parameter estimates are reported with their resulting standard deviations. @tbl-parm presents the model-estimated values and standard deviations for the main active parameters. While it’s important to consider the scale of each parameter, the results suggest that various parameters are being estimated with unusually high precision (less than 0.1).  However, four parameters have large coefficients (greater than 0.3) including the initial fishing mortality, the NCRMP ascending selectivity, the NCRMP end selectivity and the NCRMP top selectivity. 

@fig-m3-density illustrates how the estimates and uncertainty for the unfished recruitment (R0) and virgin spawning stock biomass change throughout the sequential steps of model development. In general, increasing the complexity of the model results in wider distributions, particularly for the models with the annual fishery dependent data (v28_m3, and v31_m3) and recruitment deviations estimated (v19_m3 and v31_m3). The uncertainty across the response surface for key parameters is further examined later in the report using likelihood profiles.

Stock Synthesis also provides estimates and standard deviations for derived quantities such as unfished spawning stock biomass, initial year spawning biomass, and the initial depletion. Initial depletion is defined as the initial biomass divided by the unfished biomass. @tbl-dq shows this information and it is also plotted in Figures [-@fig-m3-ratio] and [-@fig-m3-ssb].  

Compared to the other m3 model scenarios, Model v28_m3 had the lowest initial depletion reflected as the highest spawning biomass ratio (SSB Initial/SSB Unfished) (@tbl-dq and @fig-m3-dq). The sensitivity runs described later build on the exploration of uncertainty in these model scenarios.

## Jitter Analysis

Jitter analysis is a relatively simple method that can be used to assess model stability and to determine whether the search algorithm has found a global, as opposed to local, solution. The premise is that all starting values are randomly altered (or ‘jittered’) by an input constant value, and the model is rerun from the new starting values. If the resulting population trajectories across many runs converge to the same solution, this provides support that a global minimum has been obtained. This process is not fault-proof; no guarantee can ever be made that the ‘true’ solution has been found or that the model does not contain misspecification. However, if the jitter analysis results are consistent, it provides additional support that the model is performing well and has come to a stable solution. For this assessment, a jitter value of 0.2 was applied to the starting values, and 30 runs were completed. The jitter value defines a uniform distribution in cumulative normal space to generate new initial parameter values [@methot2020].

Consistent with earlier results indicating that the models reached local minima (positive definite Hessian), no jitter runs produced a lower likelihood than the best fit already identified for each model. However, with models frequently converging at higher likelihoods, the jitter analysis suggests some instability in the model scenarios (@fig-jitter). 

Comparing the Spawning stock biomass over time and the selectivity patterns across the jitter runs reveals that the jittered runs resulted on different NCRMP selectivity patterns (@fig-jitter-ssb and @fig-jitter-selectivity).

## Residual Analysis 

The primary approach to investigate model performance was a residual analysis of model fit to each data set (e.g., catch, length compositions, indices). Any temporal trend in model residuals or disproportionately high residual values can indicate model misspecification and poor performance. Ideally, residuals are randomly distributed, conform to the assumed error structure for that data source, and are not of extreme magnitude. Any extremely positive or negative residual patterns indicate poor model performance and potential unaccounted-for process or observation error.

### Catch

All models closely matched the observed 2012 - 2022 catch data, which was expected given the data-limited configurations used. These setups don’t provide much additional information beyond the catch itself, so the model has little room to estimate catch values that differ from the input data. The effect of increasing the standard error on the initial equilibrium catch to 0.3 during the model development m3 scenario was to give the model more flexibility in estimating initial equilibrium catch and corresponding initial fishing mortality. This adjustment allowed the model to explore alternative fits while remaining informed by the assumed input level of historically sustained catch. Increasing the standard error from 0.01 to 0.3 resulted in higher estimates of the initial equilibrium catch across all models, except v19 (@tbl-eqcatch). This topic will  be revisited in the sensitivity analyses, where model runs with even higher catch standard error of 2 are compared. Additional justifications for further allowing the estimated initial equilibrium catch to differ from the assumed initial equilibrium catch of 168.3 metric tons is further investigated via likelihood profiles (See @sec-daignostics-eqcatch-profile). 

### Indices

For the models without recruitment deviation being estimated (a_m2, v01_m2, and v8_m2 and v28_m3), the predicted NCRMP index is flat or shows a slight increase (Figure [-@fig-cpue2]). In the model scenarios with estimated recruitment deviations (v19_m3 and v31_m2), there is only a slightly improved fit to the index capturing a slight increase over time. Notably, high  uncertainty in the index was observed in 2021 of the NCRMP index (@fig-cpue2).

### Length Compositions

@fig-lenfit shows the cumulative fit across all years between the observed and predicted length composition by fleet for each  model. @fig-lenfit-residual shows the annual Person residuals by fleet for each model. @fig-lenfit-ncrmp provides the year-specific NRMP survey length compositions for the model scenarios that included annual fishery-independent size data (v01_m3, v08_m3, v19_m3, v28_m3, and v31_m3). Figures [-@fig-lenfit-v28] and [-@fig-lenfit-v31] provide th year-specific length compositions for the model scenarios that included annual fishery-dependent size data (v28_m3, and v31_m3).

Among the models with the annual fishery-independent size data (v01_m3, v08_m3, v19_m3, v28_m3, and v31_m3), the models with recruitment deviation being estimated (v19_m3 and v31_m2), has improved fits to the annual NCRMP length composition data (@fig-lenfit). In the scenarios without recruitment deviations, the predicted NCRMP composition is identical across years and similar to cumulative fit when the size data were aggregated in model a_m3. @fig-meanlen-ncrmp shows the observed and predicted mean length by year. In the model scenarios with estimated recruitment deviations (v19_m3 and v31_m2), there is an improved fit to the mean length capturing an increase over time.

Among the models with the annual fishery-dependent size data (v28_m3, and v31_m3), the model with recruitment deviation being estimated (v31_m3), has improved fits to the commercial length composition data (@fig-lenfit). Finally, @fig-meanlen-com shows the observed and predicted mean length by year. In the model scenarios with estimated recruitment deviations (v19_m3 and v31_m2), there is decreased error and an improved fit to the mean length capturing an increase over time.

## Retrospective Analysis

A retrospective analysis is a helpful approach for investigating the consistency of terminal year model estimates (e.g., SSB, Recruits, Fs) and is often considered a sensitivity exploration of impacts on key parameters from changes in data. The analysis sequentially removes a year of data and reruns the model. Suppose the resulting estimates of derived quantities such as SSB or recruitment differ significantly. In such a case, serial over- or underestimation of important quantities can indicate that the model has an unidentified process error and could require reassessing model assumptions. It is expected that removing data will lead to slight differences between the new terminal year estimates and the estimates for that year in the model with the complete time series of data. Estimates in years before the terminal year may have increasingly reliable information on cohort strength. Therefore, slight differences are usually expected between model runs as more years of size composition data are sequentially removed. Ideally, the difference in estimates will be slight and randomly distributed above and below the estimates from the model with complete data set time series. 

The results of a five-year retrospective analysis are plotted in @fig-retro and @fig-retro-f. All retrospectives show wide 95% confidence intervals. The retrospective pattern was most divergent in the scenarios with recruitment deviations and annual fishery-independent size data, model v31_m2. The model v19_m3 retrospective is also unusual as the models ending in 2019 and 2020 reflect an entirely different pattern overt the time series, with drastically different estimates for initial fishing mortality.

## Likelihood Profiles

Profile likelihoods are used to assess the stability of parameter estimates by examining changes in the negative log-likelihood for each data source and evaluating the influence of each source on the estimate. The analysis is performed by holding a given parameter at a constant value and rerunning the model. The model is run repeatedly over a range of reasonable parameter values. Ideally, the graph of change in likelihood values against parameter values will yield a well-defined minimum. When the profile plot shows conflicting signals or is flat across its range, the given parameter may be poorly estimated.

Typically, profiling is carried out for key parameters, particularly those defining the stock-recruit relationship (steepness, virgin recruitment, and sigma R). Profiles were explored across virgin recruitment (R0), initial equilibrium catch, and steepness.

### Unfished Recruitment (R0)

@fig-profile-r0 shows the profile likelihood for the natural log of the unfished recruitment parameter of the Beverton – Holt stock-recruit function for Puerto Rico Yellowtail Snapper across models. All models show relatively poorly defined minimums, with a range of equally plausible values reflected by only small changes in likelihood. However, with the current plots in the report, this is difficult to notice due to the large y-axis scale due to runs that converged at much higher likelihoods. @fig-profile-r0-msyspr shows the corresponding change in the Maximum Sustainable Yield Proxy based on SPR 40% across the range of unfished recruitment values explored. Across the range of R0 values explored (0.7 to 1.3 times the estimated R0 for each respective model), the estimates of the MSY proxy (based on SPR 40%) range between 100 and 300 metric tons and reflect a positive relationship with R0 (higher R0 values are associated with higher estimates of the MSY proxy).

### Initial Equilibrium Catch {#sec-daignostics-eqcatch-profile}

@fig-profile-eqcatch shows the profile likelihood for the initial equilibrium catch for Puerto Rico Yellowtail Snapper across model scenarios. The models, with the exception of v19_m3 and v28_m3, suggest improved fit around 200 metric tons of fixed initial equilibrium catch. Model v19_m3 suggests that given further flexibility the initial equilibrium may be estimated lower. This was further examined through sensitivity runs further relaxing the information that informs the initial model conditions. @fig-profile-eqcatch-msyspr shows the corresponding change in the MSY SPR 40% across the range of initial equilibrium catch values explored.

### Steepness

@fig-profile-k shows the profile likelihood for the steepness parameter of the Beverton – Holt stock-recruit function for Puerto Rico Yellowtail Snapper across models. The lowest likelihoods are not associated with the highest values of steepness. Instead they are associated with intermediately steepness values between 0.7 and 0.9. @fig-profile-k-msyspr shows the corresponding change in the MSY SPR 40% across the range of steepness values explored. 

## Sensitivity Runs

Sensitivity analyses were conducted to evaluate the impact of key model assumptions on derived quantities. Details of the process and naming conventions are provided in @tbl-ss3-dev. The analyses explored alternative assumptions for the CV on growth, fixed input for maximum age-informed mortality, and the standard error applied to catch data.

For each model scenario and sensitivity run:

-   @tbl-eqcatch provides the initial equilibrium catch
-   Tables [-@tbl-est-msy] and [-@tbl-msy] provide the MSY proxy (based on SPR 40%)
-   @tbl-msra summarizes the fishing mortality rate and spawning stock biomass ratios relative to the rate and biomass of the stock associated with the MSY proxy (based on SPR 40%)

### Growth CV

The first sensitivity scenario (s1) assumed coefficient of variation (CV) for young fish was increased from 0.15 to 0.25. The m3_s1 sensitivities resulted in a slight increase to estimated initial equilibrium catch relative to the corresponding m3 sensitivity model configurations (Tables [-@tbl-eqcatch], [-@tbl-est-msy], [-@tbl-msy], and [-@tbl-msra]). Growth is a critical process in all stock assessment models, and in this assessment, the CV for young fish was a particularly relevant sensitivity to examine due to the large number of small individuals (less than 8 cm) observed in the NCRMP fishery-independent survey length compositions. While additional sensitivities related to growth were considered, they will be revisited in the discussion section as part of the research recommendations. The current models use the best available growth parameters from @riverahernández2024.

### Natural Mortality

The second sensitivity scenario (s2) explored a slightly lower natural mortality of 0.193, corresponding to a higher maximum age of 28 years. This higher maximum age, is only slightly older than the maximum age of 26 years observed by @riverahernández2024. Although the true maximum age is often larger than the maximum age observed, particularly for species that have sustained historical fishing pressure, the @hamel2015 method estimates natural mortality based on the maximum observed age. In this assessment, age is the only factor used to inform the estimate of natural mortality, making it important to consider the implications of assuming a lower M, which reflects a less productive stock. The m3_s2 sensitivity models were similar to the corresponding m3 configurations (Tables [-@tbl-eqcatch], [-@tbl-est-msy], [-@tbl-msy], and [-@tbl-msra]).

### Standard Error on Catch

The third sensitivity scenario (s3) examined the effect of further relaxing the information that informs the initial model conditions.  In the m3 model scenarios, a standard error of 0.3 was applied to the landings data (see @sec-data-fleet-catch). Compared to the m2 model scenarios, this resulted in higher estimates of initial equilibrium catch, except for model v19_m3. The likelihood profiles (see @sec-daignostics-eqcatch-profile) for v19_m3 showed improved fit at even lower fixed estimates of equilibrium catch. This led to the exploration of increased input uncertainty using a standard error of 2.0 associated with the input equilibrium catch.

Effectively, this provides greater flexibility in estimating initial conditions. The s3 sensitivities produced similar estimates as the corresponding m2 models except for model v19a_m3_s3 which had slightly lower initial catch, slightly higher yield (Tables [-@tbl-eqcatch], [-@tbl-est-msy], [-@tbl-msy], and [-@tbl-msra]).

These results highlight the significance of uncertainty in initial conditions. This sensitivity underscores the value of longer historical data series. Without them, there is considerable uncertainty in defining the initial conditions, and the m3_s3 results imply that if early landings were larger than assumed in the m3 models, the stock may be inherently more productive.

### Standard Error on Catch and Natural Mortality

The fourth sensitivity scenario (s4) explored the combined implications of two sensitivities: increased uncertainty around initial equilibrium catch and lower natural mortality associated with higher maximum age. By evaluating both assumptions simultaneously, this scenario investigates the compounding uncertainty associated with the baseline m3 model configurations.

The combined effect of these changes were similar to the third sensitivity scenario exploring only the standard error on catch, except for model v19_m2_s4 (Tables [-@tbl-eqcatch], [-@tbl-est-msy], [-@tbl-msy], and [-@tbl-msra]). Model v19_m2_s4 resulted in the highest fishing mortality ratio (2.02) and lowest biomass ratio (0.36) across all models documented in this report.
